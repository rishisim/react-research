{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# from google import genai\n",
        "# from google.genai import types\n",
        "# # The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
        "# client = genai.Client()\n",
        "# response = client.models.generate_content(\n",
        "#     model=\"gemini-2.5-flash-lite-preview-06-17\", \n",
        "#     config=types.GenerateContentConfig(\n",
        "#         thinking_config=types.ThinkingConfig(thinking_budget=0) # Disables thinking\n",
        "#     ),\n",
        "#     contents=\"Explain how AI works in a few words\"\n",
        "# )\n",
        "# print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
        "client = genai.Client()\n",
        "\n",
        "def llm(prompt, stop=[\"\\n\"], num_traces=1):\n",
        "  temperature_setting = 0.0 if num_traces == 1 else 0.7\n",
        "  response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0), # Disables thinking\n",
        "        stop_sequences=stop,\n",
        "        temperature=temperature_setting,\n",
        "        max_output_tokens=100,\n",
        "        top_p=1.0\n",
        "    )\n",
        "  )\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_final_answer_from_trace_string(trace_trajectory_string):\n",
        "    \"\"\"\n",
        "    Extracts the final answer from a ReAct trace trajectory string.\n",
    "    Looks for the last occurrence of 'Action X: Finish[answer]'.\n",
        "    \"\"\"\n",
    "    pattern = re.compile(r\"^Action \\d+: Finish\\[(.*?)\\]\\s*$\", re.MULTILINE)\n",
    "    matches = pattern.findall(trace_trajectory_string)\n",
        "    \n",
        "    if matches:\n",
    "        # The last match in the string is the one we want\n",
    "        return matches[-1].strip()\n",
    "        \n",
        "    return None\n",
        "\n",
        "def extract_answers_from_traces(all_traces_info):\n",
        "    \"\"\"\n",
        "    Extracts the final answer from each trace in the all_traces_info list.\n",
        "    \"\"\"\n",
        "    extracted_answers = []\n",
        "    if not isinstance(all_traces_info, list):\n",
        "        print(f\"Warning: extract_answers_from_traces expected a list, got {type(all_traces_info)}\")\n",
        "        return extracted_answers\n",
        "\n",
        "    for i, trace_info in enumerate(all_traces_info):\n",
        "        trajectory = trace_info.get('traj', '')\n",
        "        answer_from_traj = extract_final_answer_from_trace_string(trajectory)\n",
        "        \n",
        "        if answer_from_traj is not None:\n",
        "            extracted_answers.append(answer_from_traj)\n",
        "        else:\n",
        "            env_answer = trace_info.get('answer')\n",
        "            if env_answer:\n",
        "                extracted_answers.append(env_answer)\n",
        "            else:\n",
        "                extracted_answers.append(None)\n",
        "    return [ans for ans in extracted_answers if ans is not None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synthesize_answer_with_llm(list_of_answers, question_for_context=\"\"):\n",
        "    \"\"\"\n",
        "    Synthesizes a single best answer from a list of answers using an LLM.\n",
        "    Includes the original question for better context if provided.\n",
        "    \"\"\"\n",
        "    if not list_of_answers:\n",
        "        return \"Error: No answers provided to synthesize.\"\n",
        "\n",
        "    unique_answers = sorted(list(set(str(a).strip() for a in list_of_answers if str(a).strip())))\n",
        "    if len(unique_answers) == 0:\n",
        "        return \"Error: No valid answers found after filtering to synthesize.\"\n",
        "    if len(unique_answers) == 1:\n",
        "        return unique_answers[0]\n",
        "\n",
        "    prompt_template = \"\"\"As an expert analyst, your task is to determine the single best answer from the following list, which was generated in response to the same question.\\n{question_context}\\nReview all answers, identify the most consistent and factually correct choice, and return that single answer. For fixed-choice questions (like yes/no or numbers), this will be a majority vote. For text-based answers, synthesize the information into the most accurate and complete response. Ignore any clear outliers or factually incorrect statements.\\n\\nGenerated Answers:\\n{formatted_answers}\\n\\nFinal Answer:\"\"\"\n",
        "\n",
        "    question_context_str = \"\"\n",
        "    if question_for_context:\n",
        "        question_context_str = f\"The question asked was: \\\"{question_for_context}\\\"\\n\\n\"\n",
        "\n",
        "    formatted_answers = \"\"\n",
        "    for i, ans in enumerate(list_of_answers):\n",
        "        formatted_answers += f\"{i+1}. {ans}\\n\"\n",
        "    formatted_answers = formatted_answers.strip()\n",
        "    \n",
        "    synthesizer_prompt = prompt_template.format(\n",
        "        question_context=question_context_str,\n",
        "        formatted_answers=formatted_answers\n",
        "    )\n",
        "    \n",
        "    final_answer = llm(synthesizer_prompt, num_traces=1)\n",
        "    return final_answer.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wikienv, wrappers\n",
        "import requests\n",
        "\n",
        "env = wikienv.WikiEnv()\n",
        "env = wrappers.HotPotQAWrapper(env, split=\"dev\")\n",
        "env = wrappers.LoggingWrapper(env)\n",
        "\n",
        "def step(env, action):\n",
        "    attempts = 0\n",
        "    while attempts < 10:\n",
        "        try:\n",
        "            return env.step(action)\n",
        "        except requests.exceptions.Timeout:\n",
        "            attempts += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ReAct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "folder = './prompts/'\n",
        "prompt_file = 'prompts_naive.json'\n",
        "with open(folder + prompt_file, 'r') as f:\n",
        "    prompt_dict = json.load(f)\n",
        "\n",
        "webthink_examples = prompt_dict['webthink_simple6']\n",
        "instruction = \"\"\"Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: \\n(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\\n(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\\n(3) Finish[answer], which returns the answer and finishes the task.\\nHere are some examples.\\n\"\"\"\n",
        "webthink_prompt_template = instruction + webthink_examples # Renamed to avoid conflict\n",
        "\n",
        "def webthink(idx=None, initial_prompt_template=webthink_prompt_template, to_print=True, num_traces=1):\n",
        "    all_traces_info = []\n",
        "    question_for_synthesis = \"\" # Define outside loop to store it\n",
        "\n",
        "    if num_traces <= 0:\n",
        "        if to_print:\n",
        "            print(f\"Warning: webthink called with num_traces = {num_traces}. Must be > 0.\")\n",
        "        return \"[INVALID_NUM_TRACES]\", [] \n",
        "\n",
        "    for trace_num in range(num_traces):\n",
        "        question = env.reset(idx=idx) # Reset environment for each trace\n",
        "        if trace_num == 0: # Capture question on first trace for synthesizer\n",
        "            question_for_synthesis = question\n",
        "        \n",
        "        current_prompt = initial_prompt_template + question + \"\\n\"\n",
        "\n",
        "        if to_print:\n",
        "            print(f\"--- Trace {trace_num + 1}/{num_traces} ---\")\n",
        "            print(idx, question)\n",
        "        \n",
        "        n_calls, n_badcalls = 0, 0\n",
        "\n",
        "        for i in range(1, 8): # Max 7 steps per trace\n",
        "            n_calls += 1\n",
        "            thought_action = llm(current_prompt + f\"Thought {i}:\", stop=[f\"\\nObservation {i}:\"], num_traces=num_traces)\n",
        "            try:\n",
        "                thought, action = thought_action.strip().split(f\"\\nAction {i}: \" )\n",
        "            except:\n",
        "                n_badcalls += 1\n",
        "                n_calls += 1\n",
        "                thought = thought_action.strip().split('\\n')[0]\n",
        "                action = llm(current_prompt + f\"Thought {i}: {thought}\\nAction {i}:\", stop=[f\"\\n\"], num_traces=num_traces).strip()\n",
        "            \n",
        "            obs, r, done, info = step(env, action[0].lower() + action[1:])\n",
        "            obs = obs.replace('\\\\n', '')\n",
        "            \n",
        "            step_str = f\"Thought {i}: {thought}\\nAction {i}: {action}\\nObservation {i}: {obs}\\n\"\n",
        "            current_prompt += step_str\n",
        "            \n",
        "            if to_print:\n",
        "                print(step_str)\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        if not done:\n",
        "            obs, r, done, info = step(env, \"finish[]\")\n",
        "            if 'traj' not in info: \n",
        "                info = {}\n",
        "            info.update({'finish_action_obs': obs})\n",
        "\n",
        "        trace_info = info.copy() \n",
        "        trace_info.update({'n_calls': n_calls, \n",
        "                           'n_badcalls': n_badcalls, \n",
        "                           'traj': current_prompt, \n",
        "                           'question_idx': idx,\n",
        "                           'question_text': question, \n",
        "                           'trace_num': trace_num + 1})\n",
        "        all_traces_info.append(trace_info)\n",
        "\n",
        "        if to_print:\n",
        "            print(f\"(Trace {trace_num + 1}) Info: {trace_info}\\n\")\n",
        "            if num_traces > 1 and trace_num < num_traces - 1:\n",
        "                print(f\"--- End of Trace {trace_num + 1} ---\\n\")\n",
        "    \n",
        "    if not all_traces_info: \n",
        "        if to_print:\n",
        "            print(\"Warning: No traces were generated despite num_traces > 0.\")\n",
        "        return \"[NO_TRACE_GENERATED]\", []\n",
        "\n",
        "    if num_traces == 1:\n",
        "        final_r = all_traces_info[0].get('reward', 0.0) \n",
        "        return final_r, all_traces_info[0] \n",
        "    \n",
        "    else: # num_traces > 1\n",
        "        if to_print:\n",
        "            print(\"\\n--- Starting Answer Synthesis ---\")\n",
        "        \n",
        "        extracted_answers = extract_answers_from_traces(all_traces_info)\n",
        "        \n",
        "        if to_print:\n",
        "            print(f\"Extracted Answers for Synthesis: {extracted_answers}\")\n",
        "\n",
        "        if not extracted_answers:\n",
        "            if to_print:\n",
        "                print(\"Warning: No answers extracted from traces. Cannot synthesize.\")\n",
        "            return \"[SYNTHESIS_FAILED_NO_EXTRACTED_ANSWERS]\", all_traces_info\n",
        "\n",
        "        synthesized_answer = synthesize_answer_with_llm(extracted_answers, question_for_synthesis)\n",
        "        \n",
        "        if to_print:\n",
        "            print(f\"Synthesized Answer: {synthesized_answer}\")\n",
        "            print(\"--- End of Answer Synthesis ---\\n\")\n",
        "\n",
        "        return synthesized_answer, all_traces_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "idxs = list(range(7405)) # Ensure idxs is defined\n",
        "\n",
        "# Example of running with multiple traces (num_traces > 1)\n",
        "print(\"--- Testing webthink with num_traces=3 ---\")\n",
        "synthesized_answer, traces_list = webthink(idx=idxs[0], to_print=True, num_traces=3)\n",
        "\n",
        "print(f\"\\n--- Synthesized Answer for idxs[0] (num_traces=3) ---\")\n",
        "print(synthesized_answer)\n",
        "print(\"---\")\n",
        "\n",
        "if isinstance(traces_list, list):\n",
        "    for i, trace_info in enumerate(traces_list):\n",
        "        print(f\"Details for Trace {i+1}:\")\n",
        "        print(f\"  Question Index: {trace_info.get('question_idx')}\")\n",
        "        print(f\"  Question Text: {trace_info.get('question_text')}\")\n",
        "        print(f\"  Trace Answer: {trace_info.get('answer')}\") \n",
        "        print(f\"  Trace Reward: {trace_info.get('reward')}\")\n",
        "        print(f\"  Trace EM: {trace_info.get('em')}\")\n",
        "        print(f\"  Trace F1: {trace_info.get('f1')}\")\n",
        "        print(\"  ---\")\n",
        "else:\n",
        "    print(\"Error: Expected a list of traces as the second part of the tuple.\")\n",
        "    print(f\"Received for traces_list: {traces_list}\")\n",
        "\n",
        "# Example of running with a single trace (num_traces = 1)\n",
        "print(\"\\n--- Testing webthink with num_traces=1 ---\")\n",
        "reward_single, info_single = webthink(idx=idxs[1], to_print=True, num_traces=1)\n",
        "\n",
        "print(f\"\\n--- Single Trace Result for idxs[1] (num_traces=1) ---\")\n",
        "print(f\"Reward: {reward_single}\")\n",
        "if info_single and isinstance(info_single, dict):\n",
        "    print(f\"  Question Index: {info_single.get('question_idx')}\")\n",
        "    print(f\"  Question Text: {info_single.get('question_text')}\")\n",
        "    print(f\"  Answer: {info_single.get('answer')}\")\n",
        "    print(f\"  EM: {info_single.get('em')}\")\n",
        "    print(f\"  F1: {info_single.get('f1')}\")\n",
        "else:\n",
        "    print(\"Error: Received no valid info_single for single trace run.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use this for benchmarking typical ReAct with Gemini Flash Lite\n",
        "import random\n",
        "import time\n",
        "idxs = list(range(7405))\n",
        "random.Random(233).shuffle(idxs)\n",
        "rs = []\n",
        "infos = []\n",
        "old_time = time.time()\n",
        "for i in idxs[:500]:\n",
        "    r_val, info_val = webthink(idx=i, to_print=False, num_traces=1) \n",
        "    rs.append(info_val['em'])\n",
        "    infos.append(info_val) \n",
        "    print(sum(rs), len(rs), sum(rs) / len(rs), (time.time() - old_time) / len(rs))\n",
        "    print('-----------')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell A: Demonstrate single trace for idxs[0]\n",
        "print(\"--- Running webthink with num_traces=1 for idxs[0] ---\")\n",
        "if 'idxs' not in globals():\n",
        "    print(\"WARNING: 'idxs' not found globally. Assuming it will be defined by a preceding cell.\")\n",
        "    print(\"For standalone execution of this cell, ensure 'idxs', 'env', 'llm', and 'webthink' are defined.\")\n",
        "    idxs = list(range(7405))\n",
        "if 'webthink' in globals() and 'env' in globals() and 'llm' in globals() and 'idxs' in globals():\n",
        "    print(f\"Using idxs[0] which is: {idxs[0]}\")\n",
        "    r_single, info_single = webthink(idx=idxs[0], to_print=True, num_traces=1)\n",
        "    print(\"\\n--- Single Trace (idxs[0]) Summary ---\")\n",
        "    print(f\"Reward: {r_single}\")\n",
        "    if info_single and isinstance(info_single, dict):\n",
        "        print(f\"Answer: {info_single.get('answer')}\")\n",
        "        print(f\"EM: {info_single.get('em')}\")\n",
        "        print(f\"F1: {info_single.get('f1')}\")\n",
        "        print(f\"Num Calls: {info_single.get('n_calls')}\")\n",
        "    else:\n",
        "        print(f\"Error: info_single was not a valid dictionary. Value: {info_single}\")\n",
        "else:\n",
        "    print(\"ERROR: One or more required components (idxs, webthink, env, llm) are not defined.\")\n",
        "    print(\"Please ensure all preceding setup cells are executed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell B: Demonstrate multiple traces for idxs[0]\n",
        "print(\"\\n--- Running webthink with num_traces=3 for idxs[0] (Updated for Synthesizer) ---\")\n",
        "if 'idxs' not in globals():\n",
        "    print(\"WARNING: 'idxs' not found globally. Assuming it will be defined by a preceding cell.\")\n",
        "    print(\"For standalone execution of this cell, ensure 'idxs', 'env', 'llm', and 'webthink' are defined.\")\n",
        "    idxs = list(range(7405))\n",
        "if 'webthink' in globals() and 'env' in globals() and 'llm' in globals() and 'idxs' in globals():\n",
        "    print(f\"Using idxs[0] which is: {idxs[0]}\")\n",
        "    \n",
        "    synthesized_answer_b, multi_traces_list_b = webthink(idx=idxs[0], to_print=True, num_traces=3)\n",
        "    \n",
        "    print(\"\\n--- Multi-Trace (idxs[0]) Summary (Updated for Synthesizer) ---\")\n",
        "    print(f\"Synthesized Answer: {synthesized_answer_b}\")\n",
        "    print(\"---\")\n",
        "    \n",
        "    if isinstance(multi_traces_list_b, list):\n",
        "        for i, trace_info in enumerate(multi_traces_list_b):\n",
        "            print(f\"Trace {i+1} Summary:\")\n",
        "            print(f\"  Question Index: {trace_info.get('question_idx')}\")\n",
        "            print(f\"  Question Text: {trace_info.get('question_text')}\") \n",
        "            print(f\"  Answer: {trace_info.get('answer')}\") \n",
        "            print(f\"  Reward: {trace_info.get('reward')}\") \n",
        "            print(f\"  EM: {trace_info.get('em')}\") \n",
        "            print(f\"  F1: {trace_info.get('f1')}\") \n",
        "            print(f\"  Num Calls: {trace_info.get('n_calls')}\")\n",
        "            print(\"  ---\")\n",
        "    else:\n",
        "        print(\"Expected a list of traces as the second element of the tuple, but received something else.\")\n",
        "        print(f\"Received for traces list: {multi_traces_list_b}\")\n",
        "else:\n",
        "    print(\"ERROR: One or more required components (idxs, webthink, env, llm) are not defined.\")\n",
        "    print(\"Please ensure all preceding setup cells are executed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
